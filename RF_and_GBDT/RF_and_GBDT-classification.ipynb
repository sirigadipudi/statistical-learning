{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ee511/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#For regression, boston dataset\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For classification, credit g dataset\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "X = np.array(X)\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X[idxs], y[idxs], idxs\n",
    "\n",
    "train, target, idxsi = bootstrap_sample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_point(train, target, lmda=0, gamma=0):\n",
    "    node =None\n",
    "    best_feature = None\n",
    "    best_val = None\n",
    "    best_gain = 0\n",
    "    num_featuress = train.shape[1]\n",
    "    num_features = np.random.choice(num_featuress, size=int(0.5*num_featuress),replace=True)\n",
    "    for feature in (num_features):\n",
    "        feature_values = train[:, feature]\n",
    "        feature_values_sorted = np.sort(feature_values)\n",
    "        num_sorted = feature_values_sorted.shape[0]\n",
    "        thresh = []\n",
    "        if type(feature_values)==[int, float]:\n",
    "\n",
    "            for j in range(num_sorted-1):\n",
    "                thresh_val = (feature_values_sorted[j] + feature_values_sorted[j+1]) / 2\n",
    "                thresh.append(thresh_val)\n",
    "\n",
    "        else: \n",
    "            for j in range(train.shape[0]):\n",
    "                thresh.append(feature_values[j])\n",
    "        for val in thresh:\n",
    "            left_child = {'X': [], 'y': []}\n",
    "            right_child = {'X': [], 'y': []}\n",
    "            left_indices =  np.where(feature_values<= val)[0]\n",
    "            right_indices = np.where(feature_values > val)[0]\n",
    "            \n",
    "            if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "                continue\n",
    "            left_target=[]\n",
    "            #left_x = []\n",
    "            right_target=[]\n",
    "            #right_x = []\n",
    "            left_target = target[left_indices]\n",
    "            left_child['X'] = train[left_indices]\n",
    "            left_child['y'] = target[left_indices]\n",
    "            \n",
    "            right_target = target[right_indices]\n",
    "            right_child['X'] = train[right_indices]\n",
    "            right_child['y'] = target[right_indices]\n",
    "        \n",
    "            G_left, H_left = compute_loss_derivatives(left_target)\n",
    "            G_right, H_right = compute_loss_derivatives(right_target)\n",
    "            gain = ((((G_left**2)/(H_left + lmda)) + ((G_right**2)/(H_right + lmda))\n",
    "                    - (((G_left + G_right)**2)/(H_left + H_right + lmda))) / 2) - gamma\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                left_child['X'] = np.array(left_child['X'])\n",
    "                right_child['X'] = np.array(right_child['X'])\n",
    "                node = {'gain': gain,\n",
    "                    'left_child': left_child,\n",
    "                    'right_child': right_child,\n",
    "                    'split_point': val,\n",
    "                    'feature_idx': feature}\n",
    "            if gain < 0:\n",
    "                break\n",
    "    return node       \n",
    "#    return best_feature, best_val\n",
    "\n",
    "def compute_g_h(target):\n",
    "    g = -2*(target - np.mean(target))\n",
    "    h = 2 #*(np.ones_like(target))\n",
    "    return g,h\n",
    "\n",
    "\n",
    "def compute_g_h_classification(target):\n",
    "    predicted = 1/ (1+np.exp(-target))\n",
    "    g = predicted - target  # first-order derivative\n",
    "    h = predicted * (1 - predicted)  # second-order derivative\n",
    "    return g, h\n",
    "\n",
    "# def compute_g_h_classification_gbdt(target, residual):\n",
    "#     predicted = 1/ (1+np.exp(-target))\n",
    "#     #g = predicted - residual  # first-order derivative\n",
    "#     #h = predicted * (1 - predicted)  # second-order derivative\n",
    "#     g = (np.exp(predicted) / (1+np.exp(predicted))) - target\n",
    "#     h = (np.exp(predicted))/((1+np.exp(predicted))**2)\n",
    "#     return g, h\n",
    "\n",
    "def compute_loss_derivatives(target):\n",
    "    g, h = compute_g_h_classification(target)\n",
    "    G = np.sum(g)\n",
    "    H = np.sum(h)\n",
    "    return G, H\n",
    "\n",
    "# get a bootstrap sample\n",
    "train, target, idxsi = bootstrap_sample(X_train, y_train)\n",
    "\n",
    "# find the best split\n",
    "best_feature = find_split_point(train, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_node(node, min_samples_split, max_depth, depth):\n",
    "    \n",
    "    left_child = node['left_child']\n",
    "    right_child = node['right_child']\n",
    "    \n",
    "    if (left_child == None ):\n",
    "        del(node['left_child'])\n",
    "        node['left_split'] = terminal_node(right_child['y'])\n",
    "    \n",
    "    if (right_child == None ):\n",
    "        del(node['right_child'])\n",
    "        node['right_split'] = terminal_node(left_child['y'])\n",
    "    \n",
    "        #del(node['right_child'])\n",
    "    del(node['left_child'])\n",
    "    del(node['right_child'])\n",
    "    \n",
    "    if len(left_child['y']) == 0 or len(right_child['y']) == 0:\n",
    "        empty_child = {'y': left_child['y'] + right_child['y']}\n",
    "        node['left_split'] = terminal_node(empty_child['y'])\n",
    "        node['right_split'] = terminal_node(empty_child['y'])\n",
    "        return\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        node['left_split'] = terminal_node(left_child['y'])\n",
    "        node['right_split'] = terminal_node(right_child['y'])\n",
    "        return node\n",
    "\n",
    "    if len(left_child['y']) <= min_samples_split:\n",
    "            node['left_split'] = terminal_node(left_child['y'])\n",
    "    else:\n",
    "        left_node = find_split_point(left_child['X'], left_child['y'])\n",
    "        \n",
    "        if left_node is None:\n",
    "            node['left_split'] = terminal_node(left_child['y'])\n",
    "        else:\n",
    "            node['left_split'] = left_node\n",
    "            split_node(node['left_split'], min_samples_split, max_depth, depth+1)\n",
    "\n",
    "    if len(right_child['y']) <= min_samples_split:\n",
    "            node['right_split'] = terminal_node(right_child['y'])\n",
    "    else:\n",
    "        right_node = find_split_point(right_child['X'], right_child['y'])\n",
    "        if right_node is None:\n",
    "            node['right_split'] = terminal_node(right_child['y'])\n",
    "        else:\n",
    "            node['right_split'] = right_node\n",
    "            split_node(node['right_split'], min_samples_split, max_depth, depth+1)\n",
    "    return node\n",
    "    \n",
    "def terminal_node(target):\n",
    "    G, H = compute_loss_derivatives(target)\n",
    "    weight = -G / (H+lmda)\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train, target, max_depth, min_samples_split):\n",
    "    root_node = find_split_point(train, target)\n",
    "    split_node(root_node, min_samples_split, max_depth, 1)\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, tmax, max_depth, min_samples_split):\n",
    "    tree_ls = list()\n",
    "    for i in range(tmax):\n",
    "        train, target, idxsi = bootstrap_sample(X_train, y_train)\n",
    "        tree = build_tree(train, target, max_depth, min_samples_split)\n",
    "        tree_ls.append(tree)\n",
    "    return tree_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree, X_test):\n",
    "    feature_idx = tree['feature_idx']\n",
    "\n",
    "    if X_test[feature_idx] <= tree['split_point']:\n",
    "        if type(tree['left_split']) == dict:\n",
    "            return predict_tree(tree['left_split'], X_test)\n",
    "        else:\n",
    "            value = tree['left_split']\n",
    "            return value\n",
    "    else:\n",
    "        if type(tree['right_split']) == dict:\n",
    "            return predict_tree(tree['right_split'], X_test)\n",
    "        else:\n",
    "            return tree['right_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(tree_ls, X_test):\n",
    "    pred_final = []\n",
    "    for i in range(len(X_test)):\n",
    "        pred_ls = 0\n",
    "        for tree in tree_ls:\n",
    "            pred = predict_tree(tree, X_test[i])\n",
    "            pred_ls +=pred\n",
    "        pred_ls = pred_ls/tmax\n",
    "        pred_ls = 1/(1+np.exp(-pred_ls))\n",
    "        if pred_ls<=0.5:\n",
    "            pred_onehot = 0\n",
    "        else:\n",
    "            pred_onehot = 1\n",
    "            \n",
    "        pred_final.append(pred_onehot)\n",
    "    return pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 28 # 5 to 50\n",
    "max_depth = 6 # 2 to 10\n",
    "min_samples_split = 25 # 1 to 50\n",
    "lmda = 5 # 0 to 10\n",
    "gamma = 0.4 # 0 to 1\n",
    "\n",
    "model = random_forest(X_train, y_train, tmax, max_depth, min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "171\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
      " 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "preds = predict_rf(model, X_test)\n",
    "print(preds)\n",
    "print(len(preds))\n",
    "print((y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1873171623163388\n",
      "rmse_train:  0.1736397240519698\n"
     ]
    }
   ],
   "source": [
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    rmse = np.sqrt((np.sum((y - pred)**2))*(1/len(y)))\n",
    "    return rmse\n",
    "\n",
    "rmse = root_mean_square_error(preds, y_test)\n",
    "print(rmse)\n",
    "\n",
    "preds_train = predict_rf(model, X_train)\n",
    "\n",
    "rmse_train = root_mean_square_error(preds_train,y_train)\n",
    "print(\"rmse_train: \",rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (Random Forest):  0.9649122807017544\n",
      "Training accuracy (Random Forest):  0.9698492462311558\n",
      "Training precision: 0.9838709677419355\n",
      "Testing precision: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    #TODO\n",
    "    num_correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_test_rf = accuracy(preds,y_test)\n",
    "print(\"Test accuracy (Random Forest): \", accuracy_test_rf)\n",
    "accuracy_train_rf = accuracy(preds_train,y_train)\n",
    "print(\"Training accuracy (Random Forest): \", accuracy_train_rf)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            false_positives += 1\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "# Calculate precision on the training and testing datasets\n",
    "train_precision = precision(y_train, preds_train)\n",
    "test_precision = precision(y_test, preds)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## GBDT ###########################################\n",
    "\n",
    "def find_split_point_gbdt(train, target,residual, lmda=0, gamma=0):\n",
    "    node =None\n",
    "    best_feature = None\n",
    "    best_val = None\n",
    "    best_gain = 0\n",
    "    num_featuress = train.shape[1]\n",
    "    num_features = np.random.choice(num_featuress, size=int(0.5*num_featuress),replace=True)\n",
    "    #for feature in enumerate(selected_features):\n",
    "    for feature in (num_features):\n",
    "        feature_values = train[:, feature]\n",
    "        feature_values_sorted = np.sort(feature_values)\n",
    "        num_sorted = feature_values_sorted.shape[0]\n",
    "        thresh = []\n",
    "        if type(feature_values)==[int, float]:\n",
    "\n",
    "            for j in range(num_sorted-1):\n",
    "                thresh_val = (feature_values_sorted[j] + feature_values_sorted[j+1]) / 2\n",
    "                thresh.append(thresh_val)\n",
    "\n",
    "        else: \n",
    "            for j in range(train.shape[0]):\n",
    "                thresh.append(feature_values[j])\n",
    "        for val in thresh:\n",
    "            left_child = {'X': [], 'y': []}\n",
    "            right_child = {'X': [], 'y': []}\n",
    "            left_indices =  np.where(feature_values<= val)[0]\n",
    "            right_indices = np.where(feature_values > val)[0]\n",
    "            \n",
    "            if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "                continue\n",
    "            left_target=[]\n",
    "            #left_x = []\n",
    "            right_target=[]\n",
    "            #right_x = []\n",
    "            left_target = target[left_indices]\n",
    "            left_child['X'] = train[left_indices]\n",
    "            left_child['y'] = target[left_indices]\n",
    "            \n",
    "            right_target = target[right_indices]\n",
    "            right_child['X'] = train[right_indices]\n",
    "            right_child['y'] = target[right_indices]\n",
    "            residual_left = residual[left_indices]\n",
    "            residual_right = residual[right_indices]\n",
    "        \n",
    "            G_left, H_left = compute_loss_derivatives_gbdt(left_target, residual_left)\n",
    "            G_right, H_right = compute_loss_derivatives_gbdt(right_target, residual_right)\n",
    "            gain = ((((G_left**2)/(H_left + lmda)) + ((G_right**2)/(H_right + lmda))\n",
    "                    - (((G_left + G_right)**2)/(H_left + H_right + lmda))) / 2) - gamma\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                left_child['X'] = np.array(left_child['X'])\n",
    "                right_child['X'] = np.array(right_child['X'])\n",
    "                node = {'gain': gain,\n",
    "                    'left_child': left_child,\n",
    "                    'right_child': right_child,\n",
    "                    'split_point': val,\n",
    "                    'feature_idx': feature,\n",
    "                    'left_indices': left_indices,\n",
    "                    'right_indices': right_indices}\n",
    "            if gain < 0:\n",
    "                break\n",
    "    return node       \n",
    "#    return best_feature, best_val\n",
    "\n",
    "def compute_g_h_gbdt(target,residual):\n",
    "    g = -2*(target - residual)\n",
    "    h = 2 \n",
    "    return g,h\n",
    "\n",
    "\n",
    "def compute_g_h_classification_gbdt(target, residual):\n",
    "    predicted = 1/ (1+np.exp(-target))\n",
    "    #g = predicted - residual  # first-order derivative\n",
    "    #h = predicted * (1 - predicted)  # second-order derivative\n",
    "    g = (np.exp(predicted) / (1+np.exp(predicted))) - target\n",
    "    h = (np.exp(predicted))/((1+np.exp(predicted))**2)\n",
    "    return g, h\n",
    "\n",
    "def compute_loss_derivatives_gbdt(target,residual):\n",
    "    g, h = compute_g_h_classification_gbdt(target,residual)\n",
    "    # g,h = compute_g_h_gbdt(target,residual)\n",
    "    G = np.sum(g)\n",
    "    H = np.sum(h)\n",
    "    return G, H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node_gbdt(node, residual, min_samples_split, max_depth, depth):\n",
    "    \n",
    "    left_child = node['left_child']\n",
    "    right_child = node['right_child']\n",
    "    left_indices = node['left_indices']\n",
    "    \n",
    "    right_indices = node['right_indices']\n",
    "    \n",
    "    if (left_child == None ):\n",
    "        del(node['left_child'])\n",
    "        node['left_split'] = terminal_node_gbdt(right_child['y'], residual[right_indices])\n",
    "    \n",
    "    if (right_child == None):\n",
    "        del(node['right_child'])\n",
    "        node['right_split'] = terminal_node_gbdt(left_child['y'], residual[left_indices])\n",
    "    \n",
    "    del(node['left_child'])\n",
    "    del(node['right_child'])\n",
    "    del(node['left_indices'])\n",
    "    del(node['right_indices'])\n",
    "    \n",
    "    if len(left_child['y']) == 0 or len(right_child['y']) == 0:\n",
    "        empty_child = {'y': left_child['y'] + right_child['y']}\n",
    "        node['left_split'] = terminal_node_gbdt(empty_child['y'],residual[left_indices])\n",
    "        node['right_split'] = terminal_node_gbdt(empty_child['y'],residual[right_indices])\n",
    "        return\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        node['left_split'] = terminal_node_gbdt(left_child['y'],residual[left_indices])\n",
    "        node['right_split'] = terminal_node_gbdt(right_child['y'],residual[right_indices])\n",
    "        return node\n",
    "\n",
    "    if len(left_child['y']) <= min_samples_split:\n",
    "            node['left_split'] = terminal_node_gbdt(left_child['y'],residual[left_indices])\n",
    "    else:\n",
    "        left_node = find_split_point_gbdt(left_child['X'], left_child['y'], residual[left_indices])\n",
    "        \n",
    "        if left_node is None:\n",
    "            node['left_split'] = terminal_node_gbdt(left_child['y'], residual[left_indices])\n",
    "        else:\n",
    "            node['left_split'] = left_node\n",
    "            split_node_gbdt(node['left_split'],residual[left_indices], min_samples_split, max_depth, depth+1)\n",
    "\n",
    "    if len(right_child['y']) <= min_samples_split:\n",
    "            node['right_split'] = terminal_node_gbdt(right_child['y'],residual[right_indices])\n",
    "    else:\n",
    "        right_node = find_split_point_gbdt(right_child['X'], right_child['y'],residual[right_indices])\n",
    "        if right_node is None:\n",
    "            node['right_split'] = terminal_node_gbdt(right_child['y'],residual[right_indices])\n",
    "        else:\n",
    "            node['right_split'] = right_node\n",
    "            split_node_gbdt(node['right_split'],residual[right_indices], min_samples_split, max_depth, depth+1)\n",
    "    return node\n",
    "    \n",
    "def terminal_node_gbdt(target,residual):\n",
    "    G, H = compute_loss_derivatives_gbdt(target,residual)\n",
    "    weight = -G / (H+lmda)\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### write\n",
    "def build_tree_gbdt(train, target, residual, max_depth, min_samples_split):\n",
    "    root_node = find_split_point_gbdt(train, target, residual)\n",
    "    split_node_gbdt(root_node, residual, min_samples_split, max_depth, 1)\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check\n",
    "def gbdt(X_train, y_train, tmax, max_depth, min_samples_split, learning_rate):\n",
    "    #residual = y_train.copy()\n",
    "    #residual = np.zeros(len(y_train), dtype=np.float64)\n",
    "    tree_ls = []\n",
    "    for i in range(tmax):\n",
    "        if i == 0:\n",
    "            residual_0 = np.mean(y_train)\n",
    "            #residual = np.float64(np.full_like(y_train, residual_0))\n",
    "            residual_0 = np.float64(residual_0)\n",
    "            residual = np.ones(y_train.size) * residual_0\n",
    "            tree = build_tree_gbdt(X_train,y_train, residual, max_depth, min_samples_split)\n",
    "            tree_ls.append(tree)\n",
    "            for j in range(len(X_train)):\n",
    "                update = predict_tree_gbdt(tree, X_train[j]) #####\n",
    "                update = np.float64(update)\n",
    "            #print(residual)\n",
    "            residual += learning_rate * update\n",
    "            \n",
    "        else:\n",
    "            tree = build_tree_gbdt(X_train,y_train, residual, max_depth, min_samples_split)\n",
    "            tree_ls.append(tree)\n",
    "            for j in range(len(X_train)):\n",
    "                update = predict_tree_gbdt(tree, X_train[j]) #######\n",
    "                update = np.float64(update)\n",
    "            residual += learning_rate * update\n",
    "    return tree_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check\n",
    "def predict_tree_gbdt(tree, X_test):\n",
    "    feature_idx = tree['feature_idx']\n",
    "\n",
    "    if X_test[feature_idx] <= tree['split_point']:\n",
    "        if type(tree['left_split']) == dict:\n",
    "            return predict_tree_gbdt(tree['left_split'], X_test)\n",
    "        else:\n",
    "            value = tree['left_split']\n",
    "            return value\n",
    "    else:\n",
    "        if type(tree['right_split']) == dict:\n",
    "            return predict_tree_gbdt(tree['right_split'], X_test)\n",
    "        else:\n",
    "            return tree['right_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### have to change\n",
    "def predict_gbdt(tree_ls, X_test):\n",
    "    pred_final = []\n",
    "    for i in range(len(X_test)):\n",
    "        pred_ls = []\n",
    "        for tree in tree_ls:\n",
    "            pred = predict_tree_gbdt(tree, X_test[i])\n",
    "            pred_ls.append(pred)\n",
    "        #pred_ls = (pred_ls/tmax)\n",
    "        pred_ls = sum(pred_ls)\n",
    "        #print(\"pred_ls before sigmoid: \",pred_ls)\n",
    "        pred_ls = 1/(1+np.exp(-pred_ls))\n",
    "        if pred_ls<=0.5:\n",
    "            pred_onehot = 0\n",
    "        else:\n",
    "            pred_onehot = 1\n",
    "        pred_final.append(pred_onehot)\n",
    "    return pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 25 # 5 to 50\n",
    "max_depth = 7 # 2 to 10\n",
    "min_samples_split = 30 # 1 to 50\n",
    "lmda = 6 # 0 to 10\n",
    "gamma = 0.5 # 0 to 1\n",
    "learning_rate = np.float64(0.5) # 0.1 to 1.0\n",
    "\n",
    "model_gbdt = gbdt(X_train, y_train, tmax, max_depth, min_samples_split, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n",
      "300\n",
      "[1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
      " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
      " 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "preds_gbdt = predict_gbdt(model_gbdt, X_test)\n",
    "print(preds_gbdt)\n",
    "print(len(preds_gbdt))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_test: 0.5354126134736337\n",
      "rmse_train:  0.42928512003762054\n"
     ]
    }
   ],
   "source": [
    "#RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    rmse = np.sqrt((np.sum((y - pred)**2))*(1/len(y)))\n",
    "    return rmse\n",
    "\n",
    "rmse_test_gbdt = root_mean_square_error(preds_gbdt, y_test)\n",
    "print(\"rmse_test:\",rmse_test_gbdt)\n",
    "\n",
    "predsgbdt_train = predict_gbdt(model_gbdt, X_train)\n",
    "\n",
    "rmsegbdt_train = root_mean_square_error(predsgbdt_train,y_train)\n",
    "print(\"rmse_train: \",rmsegbdt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7133333333333334\n",
      "Training accuracy:  0.8157142857142857\n",
      "Training precision: 0.9247058823529412\n",
      "Testing precision: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    #TODO\n",
    "    num_correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(y_true)\n",
    "    return accuracy\n",
    "    \n",
    "# Assuming you have a classification model and test data loaded\n",
    "#y_true = test_labels  # true labels of the test set\n",
    "#y_pred = my_model.predict(test_data)  # predicted labels of the test set\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_test = accuracy(preds_gbdt,y_test)\n",
    "print(\"Test accuracy: \", accuracy_test)\n",
    "accuracy_train = accuracy(predsgbdt_train,y_train)\n",
    "print(\"Training accuracy: \", accuracy_train)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            false_positives += 1\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "# Calculate precision on the training and testing datasets\n",
    "train_precision = precision(y_train, predsgbdt_train)\n",
    "test_precision = precision(y_test, preds_gbdt)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 25 # 5 to 50\n",
    "max_depth = 7 # 2 to 10\n",
    "min_samples_split = 30 # 1 to 50\n",
    "lmda = 6 # 0 to 10\n",
    "gamma = 0.5 # 0 to 1\n",
    "learning_rate = np.float64(0.5) # 0.1 to 1.0\n",
    "\n",
    "model_gbdt = gbdt(X_train, y_train, tmax, max_depth, min_samples_split, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "171\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
      " 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "preds_gbdt = predict_gbdt(model_gbdt, X_test)\n",
    "print(preds_gbdt)\n",
    "print(len(preds_gbdt))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_test: 0.1873171623163388\n",
      "rmse_train:  0.15037641213512565\n"
     ]
    }
   ],
   "source": [
    "#RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    rmse = np.sqrt((np.sum((y - pred)**2))*(1/len(y)))\n",
    "    return rmse\n",
    "\n",
    "rmse_test_gbdt = root_mean_square_error(preds_gbdt, y_test)\n",
    "print(\"rmse_test:\",rmse_test_gbdt)\n",
    "\n",
    "predsgbdt_train = predict_gbdt(model_gbdt, X_train)\n",
    "\n",
    "rmsegbdt_train = root_mean_square_error(predsgbdt_train,y_train)\n",
    "print(\"rmse_train: \",rmsegbdt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9649122807017544\n",
      "Training accuracy:  0.9773869346733668\n",
      "Training precision: 0.9879518072289156\n",
      "Testing precision: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    #TODO\n",
    "    num_correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(y_true)\n",
    "    return accuracy\n",
    "    \n",
    "# Assuming you have a classification model and test data loaded\n",
    "#y_true = test_labels  # true labels of the test set\n",
    "#y_pred = my_model.predict(test_data)  # predicted labels of the test set\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_test = accuracy(preds_gbdt,y_test)\n",
    "print(\"Test accuracy: \", accuracy_test)\n",
    "accuracy_train = accuracy(predsgbdt_train,y_train)\n",
    "print(\"Training accuracy: \", accuracy_train)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            false_positives += 1\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "# Calculate precision on the training and testing datasets\n",
    "train_precision = precision(y_train, predsgbdt_train)\n",
    "test_precision = precision(y_test, preds_gbdt)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
